{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 2 layer neural network for binary classification from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = [2,3]\n",
    "c2 = [13,15]\n",
    "no = 50\n",
    "Class1 = np.matlib.repmat(c1, no,1) + np.random.randn(no,len(c1))\n",
    "Class2 = np.matlib.repmat(c2, no,1) + np.random.randn(no,len(c2))\n",
    "Data = np.append(Class1,Class2,axis = 0)\n",
    "Trainlabel  = np.append(np.zeros((no,1)),np.ones((no,1)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb3daf8f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Class1[:,0],Class1[:,1],'ro')\n",
    "plt.plot(Class2[:,0],Class2[:,1],'bo')\n",
    "plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Data.shape[0]\n",
    "X = Data.T\n",
    "y = Trainlabel.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network parameter intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "n_i = 2\n",
    "n_h = 4\n",
    "n_l = 1\n",
    "learningrate = 0.005\n",
    "numiter = 500\n",
    "l = [n_i,n_h,n_l]\n",
    "\n",
    "W1 = np.random.randn(n_h,n_i)*0.01\n",
    "b1 = np.random.randn(n_h,1)*0.01\n",
    "W2 = np.random.randn(n_l,n_h)*0.01\n",
    "b2 = np.random.randn(n_l,1)                  \n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,play):\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    if (play == \"forward\"):\n",
    "        return z\n",
    "    elif (play ==\"backward\"):\n",
    "        return z*(1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x,play):\n",
    "    if (play==\"forward\"):\n",
    "        return np.maximum(x,0)\n",
    "    elif (play==\"backward\"):\n",
    "        x[x<=0] = 0     \n",
    "        x[x>0] = 1     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  0 th iteration => 0.700594845397\n",
      "Loss for  1 th iteration => 0.700542084205\n",
      "Loss for  2 th iteration => 0.700490605944\n",
      "Loss for  3 th iteration => 0.700440348809\n",
      "Loss for  4 th iteration => 0.700391251476\n",
      "Loss for  5 th iteration => 0.700343253085\n",
      "Loss for  6 th iteration => 0.700296293237\n",
      "Loss for  7 th iteration => 0.700250311981\n",
      "Loss for  8 th iteration => 0.700205249808\n",
      "Loss for  9 th iteration => 0.700161047642\n",
      "Loss for  10 th iteration => 0.700117646832\n",
      "Loss for  11 th iteration => 0.700074989145\n",
      "Loss for  12 th iteration => 0.70003301676\n",
      "Loss for  13 th iteration => 0.699991672261\n",
      "Loss for  14 th iteration => 0.699950898633\n",
      "Loss for  15 th iteration => 0.699910639251\n",
      "Loss for  16 th iteration => 0.699870837882\n",
      "Loss for  17 th iteration => 0.699831438676\n",
      "Loss for  18 th iteration => 0.699792386163\n",
      "Loss for  19 th iteration => 0.699753625248\n",
      "Loss for  20 th iteration => 0.699715101208\n",
      "Loss for  21 th iteration => 0.69967675969\n",
      "Loss for  22 th iteration => 0.699638546705\n",
      "Loss for  23 th iteration => 0.69960040863\n",
      "Loss for  24 th iteration => 0.699562292201\n",
      "Loss for  25 th iteration => 0.699524144514\n",
      "Loss for  26 th iteration => 0.699485913025\n",
      "Loss for  27 th iteration => 0.699447545543\n",
      "Loss for  28 th iteration => 0.699408990236\n",
      "Loss for  29 th iteration => 0.699370195626\n",
      "Loss for  30 th iteration => 0.699331110589\n",
      "Loss for  31 th iteration => 0.699291684359\n",
      "Loss for  32 th iteration => 0.699251866522\n",
      "Loss for  33 th iteration => 0.699211607023\n",
      "Loss for  34 th iteration => 0.699170856162\n",
      "Loss for  35 th iteration => 0.699129564598\n",
      "Loss for  36 th iteration => 0.69908768335\n",
      "Loss for  37 th iteration => 0.699045163798\n",
      "Loss for  38 th iteration => 0.699001957686\n",
      "Loss for  39 th iteration => 0.698958017124\n",
      "Loss for  40 th iteration => 0.698913294589\n",
      "Loss for  41 th iteration => 0.69886774293\n",
      "Loss for  42 th iteration => 0.69882131537\n",
      "Loss for  43 th iteration => 0.698773965509\n",
      "Loss for  44 th iteration => 0.698725647329\n",
      "Loss for  45 th iteration => 0.698676315196\n",
      "Loss for  46 th iteration => 0.698625923863\n",
      "Loss for  47 th iteration => 0.698574428478\n",
      "Loss for  48 th iteration => 0.698521784585\n",
      "Loss for  49 th iteration => 0.698467948128\n",
      "Loss for  50 th iteration => 0.698412875459\n",
      "Loss for  51 th iteration => 0.698356523341\n",
      "Loss for  52 th iteration => 0.698298848954\n",
      "Loss for  53 th iteration => 0.698239809898\n",
      "Loss for  54 th iteration => 0.698179364201\n",
      "Loss for  55 th iteration => 0.698117470322\n",
      "Loss for  56 th iteration => 0.698054087162\n",
      "Loss for  57 th iteration => 0.697989174063\n",
      "Loss for  58 th iteration => 0.69792269082\n",
      "Loss for  59 th iteration => 0.697854597682\n",
      "Loss for  60 th iteration => 0.697784855363\n",
      "Loss for  61 th iteration => 0.697713425047\n",
      "Loss for  62 th iteration => 0.697640268392\n",
      "Loss for  63 th iteration => 0.697565347541\n",
      "Loss for  64 th iteration => 0.697488625126\n",
      "Loss for  65 th iteration => 0.697410064273\n",
      "Loss for  66 th iteration => 0.697329628616\n",
      "Loss for  67 th iteration => 0.697247282296\n",
      "Loss for  68 th iteration => 0.697162989974\n",
      "Loss for  69 th iteration => 0.697076716835\n",
      "Loss for  70 th iteration => 0.696988428597\n",
      "Loss for  71 th iteration => 0.696898091516\n",
      "Loss for  72 th iteration => 0.696805672397\n",
      "Loss for  73 th iteration => 0.696711138598\n",
      "Loss for  74 th iteration => 0.69661445804\n",
      "Loss for  75 th iteration => 0.696515599214\n",
      "Loss for  76 th iteration => 0.696414531186\n",
      "Loss for  77 th iteration => 0.696311223607\n",
      "Loss for  78 th iteration => 0.696205646723\n",
      "Loss for  79 th iteration => 0.696097771374\n",
      "Loss for  80 th iteration => 0.695987569012\n",
      "Loss for  81 th iteration => 0.695875011701\n",
      "Loss for  82 th iteration => 0.695760072128\n",
      "Loss for  83 th iteration => 0.695642723609\n",
      "Loss for  84 th iteration => 0.695522940096\n",
      "Loss for  85 th iteration => 0.695400696186\n",
      "Loss for  86 th iteration => 0.695275969335\n",
      "Loss for  87 th iteration => 0.695146776117\n",
      "Loss for  88 th iteration => 0.695014949743\n",
      "Loss for  89 th iteration => 0.694880464153\n",
      "Loss for  90 th iteration => 0.694743294013\n",
      "Loss for  91 th iteration => 0.69460341472\n",
      "Loss for  92 th iteration => 0.694460802415\n",
      "Loss for  93 th iteration => 0.694315433986\n",
      "Loss for  94 th iteration => 0.694167287078\n",
      "Loss for  95 th iteration => 0.694016340098\n",
      "Loss for  96 th iteration => 0.693862572228\n",
      "Loss for  97 th iteration => 0.693705963426\n",
      "Loss for  98 th iteration => 0.693546494435\n",
      "Loss for  99 th iteration => 0.693384146792\n",
      "Loss for  100 th iteration => 0.693218902831\n",
      "Loss for  101 th iteration => 0.693050745694\n",
      "Loss for  102 th iteration => 0.692879659333\n",
      "Loss for  103 th iteration => 0.692705628519\n",
      "Loss for  104 th iteration => 0.692528638846\n",
      "Loss for  105 th iteration => 0.692348676738\n",
      "Loss for  106 th iteration => 0.692165729455\n",
      "Loss for  107 th iteration => 0.691979785097\n",
      "Loss for  108 th iteration => 0.691790832608\n",
      "Loss for  109 th iteration => 0.691598861785\n",
      "Loss for  110 th iteration => 0.691403863276\n",
      "Loss for  111 th iteration => 0.691205828591\n",
      "Loss for  112 th iteration => 0.691004750102\n",
      "Loss for  113 th iteration => 0.690800621047\n",
      "Loss for  114 th iteration => 0.690593435534\n",
      "Loss for  115 th iteration => 0.690383188547\n",
      "Loss for  116 th iteration => 0.690169875942\n",
      "Loss for  117 th iteration => 0.689953494456\n",
      "Loss for  118 th iteration => 0.689734041706\n",
      "Loss for  119 th iteration => 0.689511516193\n",
      "Loss for  120 th iteration => 0.689285917301\n",
      "Loss for  121 th iteration => 0.689057245301\n",
      "Loss for  122 th iteration => 0.688825501349\n",
      "Loss for  123 th iteration => 0.68859068749\n",
      "Loss for  124 th iteration => 0.688352806655\n",
      "Loss for  125 th iteration => 0.688111862664\n",
      "Loss for  126 th iteration => 0.687867860224\n",
      "Loss for  127 th iteration => 0.687620804927\n",
      "Loss for  128 th iteration => 0.687370703251\n",
      "Loss for  129 th iteration => 0.687117562558\n",
      "Loss for  130 th iteration => 0.686861391094\n",
      "Loss for  131 th iteration => 0.68660219798\n",
      "Loss for  132 th iteration => 0.68633999322\n",
      "Loss for  133 th iteration => 0.686074787687\n",
      "Loss for  134 th iteration => 0.685806593131\n",
      "Loss for  135 th iteration => 0.685535422164\n",
      "Loss for  136 th iteration => 0.685261288265\n",
      "Loss for  137 th iteration => 0.684984205771\n",
      "Loss for  138 th iteration => 0.684704189874\n",
      "Loss for  139 th iteration => 0.684421256615\n",
      "Loss for  140 th iteration => 0.684135422879\n",
      "Loss for  141 th iteration => 0.683846706388\n",
      "Loss for  142 th iteration => 0.683555125699\n",
      "Loss for  143 th iteration => 0.68326070019\n",
      "Loss for  144 th iteration => 0.682963450061\n",
      "Loss for  145 th iteration => 0.68266339632\n",
      "Loss for  146 th iteration => 0.682360560782\n",
      "Loss for  147 th iteration => 0.682054966054\n",
      "Loss for  148 th iteration => 0.681746635532\n",
      "Loss for  149 th iteration => 0.681435593389\n",
      "Loss for  150 th iteration => 0.68112186457\n",
      "Loss for  151 th iteration => 0.680805474777\n",
      "Loss for  152 th iteration => 0.680486450463\n",
      "Loss for  153 th iteration => 0.680164818821\n",
      "Loss for  154 th iteration => 0.679840607776\n",
      "Loss for  155 th iteration => 0.679513845969\n",
      "Loss for  156 th iteration => 0.67918456275\n",
      "Loss for  157 th iteration => 0.678852788167\n",
      "Loss for  158 th iteration => 0.678518552951\n",
      "Loss for  159 th iteration => 0.678181888507\n",
      "Loss for  160 th iteration => 0.677842826902\n",
      "Loss for  161 th iteration => 0.67750140085\n",
      "Loss for  162 th iteration => 0.677157643701\n",
      "Loss for  163 th iteration => 0.676811589428\n",
      "Loss for  164 th iteration => 0.676463272612\n",
      "Loss for  165 th iteration => 0.676112728432\n",
      "Loss for  166 th iteration => 0.675759992647\n",
      "Loss for  167 th iteration => 0.675405101584\n",
      "Loss for  168 th iteration => 0.675048092125\n",
      "Loss for  169 th iteration => 0.67468900169\n",
      "Loss for  170 th iteration => 0.674327868223\n",
      "Loss for  171 th iteration => 0.673964730179\n",
      "Loss for  172 th iteration => 0.673599626506\n",
      "Loss for  173 th iteration => 0.673232596633\n",
      "Loss for  174 th iteration => 0.672863680451\n",
      "Loss for  175 th iteration => 0.6724929183\n",
      "Loss for  176 th iteration => 0.672120350953\n",
      "Loss for  177 th iteration => 0.671746019598\n",
      "Loss for  178 th iteration => 0.671369965826\n",
      "Loss for  179 th iteration => 0.67099223161\n",
      "Loss for  180 th iteration => 0.670612859294\n",
      "Loss for  181 th iteration => 0.670231891571\n",
      "Loss for  182 th iteration => 0.669849371471\n",
      "Loss for  183 th iteration => 0.669465342343\n",
      "Loss for  184 th iteration => 0.669079847839\n",
      "Loss for  185 th iteration => 0.668692931896\n",
      "Loss for  186 th iteration => 0.66830463872\n",
      "Loss for  187 th iteration => 0.667915024808\n",
      "Loss for  188 th iteration => 0.667515084983\n",
      "Loss for  189 th iteration => 0.667113700801\n",
      "Loss for  190 th iteration => 0.666710918646\n",
      "Loss for  191 th iteration => 0.666306785156\n",
      "Loss for  192 th iteration => 0.665901347209\n",
      "Loss for  193 th iteration => 0.665494651903\n",
      "Loss for  194 th iteration => 0.665086746536\n",
      "Loss for  195 th iteration => 0.664677678592\n",
      "Loss for  196 th iteration => 0.664267495717\n",
      "Loss for  197 th iteration => 0.663856245711\n",
      "Loss for  198 th iteration => 0.663443976498\n",
      "Loss for  199 th iteration => 0.663030736119\n",
      "Loss for  200 th iteration => 0.662616572708\n",
      "Loss for  201 th iteration => 0.662201534475\n",
      "Loss for  202 th iteration => 0.661785669693\n",
      "Loss for  203 th iteration => 0.661369026673\n",
      "Loss for  204 th iteration => 0.660951653754\n",
      "Loss for  205 th iteration => 0.660533599282\n",
      "Loss for  206 th iteration => 0.660114911594\n",
      "Loss for  207 th iteration => 0.659695639\n",
      "Loss for  208 th iteration => 0.659275829768\n",
      "Loss for  209 th iteration => 0.658855532106\n",
      "Loss for  210 th iteration => 0.658434794148\n",
      "Loss for  211 th iteration => 0.658013663935\n",
      "Loss for  212 th iteration => 0.657592189398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  213 th iteration => 0.657170418349\n",
      "Loss for  214 th iteration => 0.656748398457\n",
      "Loss for  215 th iteration => 0.656326177237\n",
      "Loss for  216 th iteration => 0.655903802036\n",
      "Loss for  217 th iteration => 0.655481320015\n",
      "Loss for  218 th iteration => 0.655058778135\n",
      "Loss for  219 th iteration => 0.654636223146\n",
      "Loss for  220 th iteration => 0.654213701567\n",
      "Loss for  221 th iteration => 0.653791259677\n",
      "Loss for  222 th iteration => 0.653368943499\n",
      "Loss for  223 th iteration => 0.652946798788\n",
      "Loss for  224 th iteration => 0.652524871016\n",
      "Loss for  225 th iteration => 0.652103205362\n",
      "Loss for  226 th iteration => 0.651681846696\n",
      "Loss for  227 th iteration => 0.651260839569\n",
      "Loss for  228 th iteration => 0.650840228201\n",
      "Loss for  229 th iteration => 0.65042005647\n",
      "Loss for  230 th iteration => 0.650000367898\n",
      "Loss for  231 th iteration => 0.649581205642\n",
      "Loss for  232 th iteration => 0.649162612483\n",
      "Loss for  233 th iteration => 0.648744630817\n",
      "Loss for  234 th iteration => 0.648327302641\n",
      "Loss for  235 th iteration => 0.647910669547\n",
      "Loss for  236 th iteration => 0.64749477271\n",
      "Loss for  237 th iteration => 0.647079652881\n",
      "Loss for  238 th iteration => 0.646665350377\n",
      "Loss for  239 th iteration => 0.646251905073\n",
      "Loss for  240 th iteration => 0.645839356392\n",
      "Loss for  241 th iteration => 0.645427743299\n",
      "Loss for  242 th iteration => 0.645017104293\n",
      "Loss for  243 th iteration => 0.644607477401\n",
      "Loss for  244 th iteration => 0.644198900168\n",
      "Loss for  245 th iteration => 0.643791409652\n",
      "Loss for  246 th iteration => 0.64338504242\n",
      "Loss for  247 th iteration => 0.642979834537\n",
      "Loss for  248 th iteration => 0.642575821565\n",
      "Loss for  249 th iteration => 0.642173038555\n",
      "Loss for  250 th iteration => 0.641771520045\n",
      "Loss for  251 th iteration => 0.641371300049\n",
      "Loss for  252 th iteration => 0.64097241206\n",
      "Loss for  253 th iteration => 0.64057488904\n",
      "Loss for  254 th iteration => 0.640178763421\n",
      "Loss for  255 th iteration => 0.639784067098\n",
      "Loss for  256 th iteration => 0.639390831427\n",
      "Loss for  257 th iteration => 0.63899908722\n",
      "Loss for  258 th iteration => 0.638608864747\n",
      "Loss for  259 th iteration => 0.63822019373\n",
      "Loss for  260 th iteration => 0.63783310334\n",
      "Loss for  261 th iteration => 0.637447622198\n",
      "Loss for  262 th iteration => 0.637063778374\n",
      "Loss for  263 th iteration => 0.63668159938\n",
      "Loss for  264 th iteration => 0.636301112177\n",
      "Loss for  265 th iteration => 0.635922343167\n",
      "Loss for  266 th iteration => 0.635545318198\n",
      "Loss for  267 th iteration => 0.635170062559\n",
      "Loss for  268 th iteration => 0.634796600983\n",
      "Loss for  269 th iteration => 0.634424957647\n",
      "Loss for  270 th iteration => 0.634055156172\n",
      "Loss for  271 th iteration => 0.633687219622\n",
      "Loss for  272 th iteration => 0.633321170507\n",
      "Loss for  273 th iteration => 0.632957030785\n",
      "Loss for  274 th iteration => 0.632594821858\n",
      "Loss for  275 th iteration => 0.63223456458\n",
      "Loss for  276 th iteration => 0.631876279256\n",
      "Loss for  277 th iteration => 0.631519985642\n",
      "Loss for  278 th iteration => 0.63116570295\n",
      "Loss for  279 th iteration => 0.630813449849\n",
      "Loss for  280 th iteration => 0.630463244467\n",
      "Loss for  281 th iteration => 0.630115104394\n",
      "Loss for  282 th iteration => 0.629769046686\n",
      "Loss for  283 th iteration => 0.629425087866\n",
      "Loss for  284 th iteration => 0.629083243928\n",
      "Loss for  285 th iteration => 0.628743530341\n",
      "Loss for  286 th iteration => 0.62840596205\n",
      "Loss for  287 th iteration => 0.628070553483\n",
      "Loss for  288 th iteration => 0.627737318552\n",
      "Loss for  289 th iteration => 0.627406270657\n",
      "Loss for  290 th iteration => 0.627077422692\n",
      "Loss for  291 th iteration => 0.626750787047\n",
      "Loss for  292 th iteration => 0.626426375611\n",
      "Loss for  293 th iteration => 0.626104199782\n",
      "Loss for  294 th iteration => 0.625784270464\n",
      "Loss for  295 th iteration => 0.625466598077\n",
      "Loss for  296 th iteration => 0.625151192558\n",
      "Loss for  297 th iteration => 0.624838063368\n",
      "Loss for  298 th iteration => 0.624527219497\n",
      "Loss for  299 th iteration => 0.624218669465\n",
      "Loss for  300 th iteration => 0.623912421333\n",
      "Loss for  301 th iteration => 0.623608482702\n",
      "Loss for  302 th iteration => 0.623306860723\n",
      "Loss for  303 th iteration => 0.623007562098\n",
      "Loss for  304 th iteration => 0.622710593089\n",
      "Loss for  305 th iteration => 0.622415959518\n",
      "Loss for  306 th iteration => 0.622123666779\n",
      "Loss for  307 th iteration => 0.621833719838\n",
      "Loss for  308 th iteration => 0.621546123242\n",
      "Loss for  309 th iteration => 0.62126088112\n",
      "Loss for  310 th iteration => 0.620977997193\n",
      "Loss for  311 th iteration => 0.620697474777\n",
      "Loss for  312 th iteration => 0.620419316789\n",
      "Loss for  313 th iteration => 0.620143525753\n",
      "Loss for  314 th iteration => 0.619870103805\n",
      "Loss for  315 th iteration => 0.619599052698\n",
      "Loss for  316 th iteration => 0.619330373809\n",
      "Loss for  317 th iteration => 0.619064068144\n",
      "Loss for  318 th iteration => 0.618800136343\n",
      "Loss for  319 th iteration => 0.618538578685\n",
      "Loss for  320 th iteration => 0.618279395098\n",
      "Loss for  321 th iteration => 0.618022585158\n",
      "Loss for  322 th iteration => 0.617768148098\n",
      "Loss for  323 th iteration => 0.617516082815\n",
      "Loss for  324 th iteration => 0.617266387874\n",
      "Loss for  325 th iteration => 0.617019061512\n",
      "Loss for  326 th iteration => 0.616774101646\n",
      "Loss for  327 th iteration => 0.616531505878\n",
      "Loss for  328 th iteration => 0.6162912715\n",
      "Loss for  329 th iteration => 0.6160533955\n",
      "Loss for  330 th iteration => 0.615817874565\n",
      "Loss for  331 th iteration => 0.615584705092\n",
      "Loss for  332 th iteration => 0.615353883188\n",
      "Loss for  333 th iteration => 0.615125404678\n",
      "Loss for  334 th iteration => 0.61489926511\n",
      "Loss for  335 th iteration => 0.614675459759\n",
      "Loss for  336 th iteration => 0.614453983635\n",
      "Loss for  337 th iteration => 0.614234831486\n",
      "Loss for  338 th iteration => 0.614017997804\n",
      "Loss for  339 th iteration => 0.61380347683\n",
      "Loss for  340 th iteration => 0.613591262561\n",
      "Loss for  341 th iteration => 0.613381348751\n",
      "Loss for  342 th iteration => 0.61317372892\n",
      "Loss for  343 th iteration => 0.612968396357\n",
      "Loss for  344 th iteration => 0.612765344126\n",
      "Loss for  345 th iteration => 0.612564565071\n",
      "Loss for  346 th iteration => 0.61236605182\n",
      "Loss for  347 th iteration => 0.61216979679\n",
      "Loss for  348 th iteration => 0.611975792193\n",
      "Loss for  349 th iteration => 0.611784030038\n",
      "Loss for  350 th iteration => 0.611594502141\n",
      "Loss for  351 th iteration => 0.611407200123\n",
      "Loss for  352 th iteration => 0.611222115421\n",
      "Loss for  353 th iteration => 0.611039239287\n",
      "Loss for  354 th iteration => 0.610858562796\n",
      "Loss for  355 th iteration => 0.610680076849\n",
      "Loss for  356 th iteration => 0.610503772181\n",
      "Loss for  357 th iteration => 0.610329639359\n",
      "Loss for  358 th iteration => 0.61015766879\n",
      "Loss for  359 th iteration => 0.609987850726\n",
      "Loss for  360 th iteration => 0.609820175269\n",
      "Loss for  361 th iteration => 0.60965463237\n",
      "Loss for  362 th iteration => 0.609491211838\n",
      "Loss for  363 th iteration => 0.609329903343\n",
      "Loss for  364 th iteration => 0.60917069642\n",
      "Loss for  365 th iteration => 0.609013580471\n",
      "Loss for  366 th iteration => 0.608858544772\n",
      "Loss for  367 th iteration => 0.608705578476\n",
      "Loss for  368 th iteration => 0.608554670614\n",
      "Loss for  369 th iteration => 0.608405810102\n",
      "Loss for  370 th iteration => 0.608258985745\n",
      "Loss for  371 th iteration => 0.608114186238\n",
      "Loss for  372 th iteration => 0.607971400171\n",
      "Loss for  373 th iteration => 0.607830616034\n",
      "Loss for  374 th iteration => 0.607691822216\n",
      "Loss for  375 th iteration => 0.607555007016\n",
      "Loss for  376 th iteration => 0.607420158638\n",
      "Loss for  377 th iteration => 0.6072872652\n",
      "Loss for  378 th iteration => 0.607156314735\n",
      "Loss for  379 th iteration => 0.607027295197\n",
      "Loss for  380 th iteration => 0.606900194459\n",
      "Loss for  381 th iteration => 0.606775000322\n",
      "Loss for  382 th iteration => 0.606651700513\n",
      "Loss for  383 th iteration => 0.606530282692\n",
      "Loss for  384 th iteration => 0.606410734453\n",
      "Loss for  385 th iteration => 0.606293043328\n",
      "Loss for  386 th iteration => 0.606177196789\n",
      "Loss for  387 th iteration => 0.606063182251\n",
      "Loss for  388 th iteration => 0.605950987075\n",
      "Loss for  389 th iteration => 0.605840598573\n",
      "Loss for  390 th iteration => 0.605732004006\n",
      "Loss for  391 th iteration => 0.60562519059\n",
      "Loss for  392 th iteration => 0.605520145499\n",
      "Loss for  393 th iteration => 0.605416855865\n",
      "Loss for  394 th iteration => 0.605315308785\n",
      "Loss for  395 th iteration => 0.605215491318\n",
      "Loss for  396 th iteration => 0.605117390491\n",
      "Loss for  397 th iteration => 0.605020993302\n",
      "Loss for  398 th iteration => 0.604926286719\n",
      "Loss for  399 th iteration => 0.604833257686\n",
      "Loss for  400 th iteration => 0.604741893124\n",
      "Loss for  401 th iteration => 0.60465217993\n",
      "Loss for  402 th iteration => 0.604564104987\n",
      "Loss for  403 th iteration => 0.604477655157\n",
      "Loss for  404 th iteration => 0.604392817292\n",
      "Loss for  405 th iteration => 0.604309578228\n",
      "Loss for  406 th iteration => 0.604227924792\n",
      "Loss for  407 th iteration => 0.604147843805\n",
      "Loss for  408 th iteration => 0.604069322079\n",
      "Loss for  409 th iteration => 0.603992346424\n",
      "Loss for  410 th iteration => 0.603916903646\n",
      "Loss for  411 th iteration => 0.603842980553\n",
      "Loss for  412 th iteration => 0.603770563954\n",
      "Loss for  413 th iteration => 0.603699640659\n",
      "Loss for  414 th iteration => 0.603630197486\n",
      "Loss for  415 th iteration => 0.603562221261\n",
      "Loss for  416 th iteration => 0.603495698815\n",
      "Loss for  417 th iteration => 0.603430616993\n",
      "Loss for  418 th iteration => 0.60336696265\n",
      "Loss for  419 th iteration => 0.603304722656\n",
      "Loss for  420 th iteration => 0.603243883897\n",
      "Loss for  421 th iteration => 0.603184433273\n",
      "Loss for  422 th iteration => 0.603126357707\n",
      "Loss for  423 th iteration => 0.603069644138\n",
      "Loss for  424 th iteration => 0.603014279529\n",
      "Loss for  425 th iteration => 0.602960250865\n",
      "Loss for  426 th iteration => 0.602907545156\n",
      "Loss for  427 th iteration => 0.602856149438\n",
      "Loss for  428 th iteration => 0.602806050773\n",
      "Loss for  429 th iteration => 0.602757236252\n",
      "Loss for  430 th iteration => 0.602709692998\n",
      "Loss for  431 th iteration => 0.602663408162\n",
      "Loss for  432 th iteration => 0.602618368929\n",
      "Loss for  433 th iteration => 0.602574562519\n",
      "Loss for  434 th iteration => 0.602531976183\n",
      "Loss for  435 th iteration => 0.602490597212\n",
      "Loss for  436 th iteration => 0.602450412932\n",
      "Loss for  437 th iteration => 0.602411410709\n",
      "Loss for  438 th iteration => 0.602373577948\n",
      "Loss for  439 th iteration => 0.602336902092\n",
      "Loss for  440 th iteration => 0.602301370629\n",
      "Loss for  441 th iteration => 0.602266971089\n",
      "Loss for  442 th iteration => 0.602233691044\n",
      "Loss for  443 th iteration => 0.602201518113\n",
      "Loss for  444 th iteration => 0.602170439958\n",
      "Loss for  445 th iteration => 0.60214044429\n",
      "Loss for  446 th iteration => 0.602111518866\n",
      "Loss for  447 th iteration => 0.602083651492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  448 th iteration => 0.602056830024\n",
      "Loss for  449 th iteration => 0.602031042366\n",
      "Loss for  450 th iteration => 0.602006276476\n",
      "Loss for  451 th iteration => 0.601982520361\n",
      "Loss for  452 th iteration => 0.601959762083\n",
      "Loss for  453 th iteration => 0.601937989755\n",
      "Loss for  454 th iteration => 0.601917191546\n",
      "Loss for  455 th iteration => 0.60189735568\n",
      "Loss for  456 th iteration => 0.601878470434\n",
      "Loss for  457 th iteration => 0.601860524146\n",
      "Loss for  458 th iteration => 0.601843505206\n",
      "Loss for  459 th iteration => 0.601827402065\n",
      "Loss for  460 th iteration => 0.601812203231\n",
      "Loss for  461 th iteration => 0.601797897272\n",
      "Loss for  462 th iteration => 0.601784472813\n",
      "Loss for  463 th iteration => 0.601771918543\n",
      "Loss for  464 th iteration => 0.601760223208\n",
      "Loss for  465 th iteration => 0.601749375618\n",
      "Loss for  466 th iteration => 0.601739364644\n",
      "Loss for  467 th iteration => 0.601730179218\n",
      "Loss for  468 th iteration => 0.601721808336\n",
      "Loss for  469 th iteration => 0.601714241058\n",
      "Loss for  470 th iteration => 0.601707466507\n",
      "Loss for  471 th iteration => 0.60170147387\n",
      "Loss for  472 th iteration => 0.601696252399\n",
      "Loss for  473 th iteration => 0.601691791412\n",
      "Loss for  474 th iteration => 0.601688080292\n",
      "Loss for  475 th iteration => 0.601685108487\n",
      "Loss for  476 th iteration => 0.601682865513\n",
      "Loss for  477 th iteration => 0.601681340951\n",
      "Loss for  478 th iteration => 0.60168052445\n",
      "Loss for  479 th iteration => 0.601680405727\n",
      "Loss for  480 th iteration => 0.601680974565\n",
      "Loss for  481 th iteration => 0.601682220818\n",
      "Loss for  482 th iteration => 0.601684134405\n",
      "Loss for  483 th iteration => 0.601686705315\n",
      "Loss for  484 th iteration => 0.601689923607\n",
      "Loss for  485 th iteration => 0.601693779407\n",
      "Loss for  486 th iteration => 0.601698262913\n",
      "Loss for  487 th iteration => 0.601703364389\n",
      "Loss for  488 th iteration => 0.601709074174\n",
      "Loss for  489 th iteration => 0.601715382672\n",
      "Loss for  490 th iteration => 0.601722280359\n",
      "Loss for  491 th iteration => 0.601729757783\n",
      "Loss for  492 th iteration => 0.601737805561\n",
      "Loss for  493 th iteration => 0.601746414381\n",
      "Loss for  494 th iteration => 0.601755575002\n",
      "Loss for  495 th iteration => 0.601765278255\n",
      "Loss for  496 th iteration => 0.601775515039\n",
      "Loss for  497 th iteration => 0.601786276328\n",
      "Loss for  498 th iteration => 0.601797553166\n",
      "Loss for  499 th iteration => 0.601809336668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(numiter):\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = relu(Z1,play = \"forward\")\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2,play = \"forward\")\n",
    "   \n",
    "    L = (1.0/m) * -np.sum(np.multiply(y,np.log(A2)) + np.multiply(1-y,np.log(1-A2)))\n",
    "    \n",
    "    dZ2 = A2 - y\n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),relu(A1,play=\"backward\"))\n",
    "    dW2 = (1.0/m) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1.0/m) * np.sum(dZ2,axis = 1,keepdims = True)\n",
    "    dW1 = (1.0/m) * np.dot(dZ1,X.T)\n",
    "    db1 = (1.0/m) * np.sum(dZ1,axis = 1,keepdims = True)\n",
    "    W1 = W1 - learningrate * dW1\n",
    "    b1 = b1 - learningrate * db1\n",
    "    W2 = W2 - learningrate * dW2\n",
    "    b2 = b2 - learningrate* db2\n",
    "    print \"Loss for \",i,\"th iteration =>\",L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predictions\n",
    "\n",
    "\n",
    "def prediction(X,W1,b1,W2,b2):\n",
    "    Z1pred = np.dot(W1,X) + b1\n",
    "    A1pred = relu(Z1pred,play = \"forward\")\n",
    "    Z2pred = np.dot(W2,A1pred) + b2\n",
    "    A2pred = sigmoid(Z2pred,play = \"forward\")\n",
    "    prediction = []\n",
    "\n",
    "\n",
    "    for i in range(A2pred.shape[1]):\n",
    "        if (A2pred[0][i]>0.5):\n",
    "            prediction.append(1)\n",
    "        elif (A2pred[0][i] <= 0.5):\n",
    "            prediction.append(0)\n",
    "    N = len(prediction)\n",
    "    prediction = np.array(prediction)\n",
    "    prediction =prediction.reshape(1,N)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTrain = prediction(X,W1,b1,W2,b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truelabel = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Label and True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction label for traindata: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "True label for train data: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print \"prediction label for traindata:\",predictionTrain\n",
    "print \"True label for train data:\", Truelabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tc1 = [3,4]\n",
    "Tc2 = [14,16]\n",
    "TClass1 = np.matlib.repmat(Tc1, no,1) + np.random.randn(no,len(c1))\n",
    "TClass2 = np.matlib.repmat(Tc2, no,1) + np.random.randn(no,len(c2))\n",
    "TData = np.append(TClass1,TClass2,axis = 0)\n",
    "Testlabel  = np.append(np.zeros((no,1)),np.ones((no,1)),axis = 0)\n",
    "X2 = TData.T\n",
    "y2 = Testlabel.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb3b68b110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(TClass1[:,0],TClass1[:,1],'ro')\n",
    "plt.plot(TClass2[:,0],TClass2[:,1],'bo')\n",
    "\n",
    "plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction label for testndata: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "True label for test data: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "predictionTest = prediction(X2,W1,b1,W2,b2)\n",
    "print \"prediction label for testndata:\",predictionTest\n",
    "print \"True label for test data:\",y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(predictionTest)\n",
    "y_pred = y_pred.astype(int)\n",
    "y_true = np.array(y2)\n",
    "y_true = y_true.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true[0],y_pred[0])\n",
    "cm = cm.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
    "print \"Accuracy:\",Accuracy*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 50.,   0.],\n",
       "       [  0.,  50.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
